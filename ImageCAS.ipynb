{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7086150,
          "sourceType": "datasetVersion",
          "datasetId": 3937305
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ImageCAS",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheWitcher41/ER_SYS/blob/main/ImageCAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "xiaoweixumedicalai_imagecas_path = kagglehub.dataset_download('xiaoweixumedicalai/imagecas')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "y0C9aq5cw5tq"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T05:05:39.212233Z",
          "iopub.execute_input": "2025-02-16T05:05:39.212433Z",
          "iopub.status.idle": "2025-02-16T05:05:40.274894Z",
          "shell.execute_reply.started": "2025-02-16T05:05:39.212414Z",
          "shell.execute_reply": "2025-02-16T05:05:40.273772Z"
        },
        "id": "LUn7lkrtw5ts"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "dataset_path = \"/kaggle/input/imagecas\"\n",
        "combined_zip_path = \"/kaggle/working/combined.zip\"\n",
        "extracted_path = \"/kaggle/working/extracted\"\n",
        "\n",
        "archive_parts = [\n",
        "    \"1-200.z01\", \"1-200.z02\", \"1-200.z03\", \"1-200.z04\", \"1-200.change2zip\",\n",
        "    \"201-400.z01\", \"201-400.z02\", \"201-400.z03\", \"201-400.z04\", \"201-400.change2zip\",\n",
        "    \"401-600.z01\", \"401-600.z02\", \"401-600.z03\", \"401-600.z04\", \"401-600.change2zip\",\n",
        "    \"601-800.z01\", \"601-800.z02\", \"601-800.z03\", \"601-800.z04\", \"601-800.change2zip\",\n",
        "    \"801-1000.z01\", \"801-1000.z02\", \"801-1000.z03\", \"801-1000.z04\", \"801-1000.change2zip\"\n",
        "]\n",
        "\n",
        "print(\"Combining multi-part files...\")\n",
        "with open(combined_zip_path, \"wb\") as combined_zip:\n",
        "    for part in archive_parts:\n",
        "        part_path = os.path.join(dataset_path, part)\n",
        "        if os.path.exists(part_path):\n",
        "            print(f\"Adding {part}...\")\n",
        "            with open(part_path, \"rb\") as part_file:\n",
        "                combined_zip.write(part_file.read())\n",
        "        else:\n",
        "            print(f\"Warning: {part} not found. Skipping...\")\n",
        "\n",
        "print(\"Extracting the combined ZIP file...\")\n",
        "os.makedirs(extracted_path, exist_ok=True)\n",
        "try:\n",
        "    with zipfile.ZipFile(combined_zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "    print(\"Extraction complete!\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: The combined file is not a valid ZIP file. Please check the multi-part files.\")\n",
        "\n",
        "print(\"Verifying extracted files...\")\n",
        "extracted_files = os.listdir(extracted_path)\n",
        "print(f\"Extracted {len(extracted_files)} files/folders:\")\n",
        "for file in extracted_files:\n",
        "    print(f\"- {file}\")\n",
        "\n",
        "print(\"Cleaning up...\")\n",
        "if os.path.exists(combined_zip_path):\n",
        "    os.remove(combined_zip_path)\n",
        "    print(f\"Deleted {combined_zip_path} to free up space.\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "IxmZJmbCw5tt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_files = os.listdir(extracted_path)\n",
        "print(\"Extracted files/folders:\", extracted_files)\n",
        "\n",
        "for root, dirs, files in os.walk(extracted_path):\n",
        "    print(f\"Directory: {root}\")\n",
        "    print(f\"Subdirectories: {dirs}\")\n",
        "    print(f\"Files: {files}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FgsHoyd3w5tt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = os.path.join(extracted_path, \"example_image.jpg\")\n",
        "image = Image.open(image_path)\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "J0WKdxkkw5tt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = os.path.join(extracted_path, \"Coronary_Segmentation_deep_learning/data_list/split_1000.csv\")\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "PaUps0Tew5tt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision opencv-python pandas scikit-image pyyaml"
      ],
      "metadata": {
        "trusted": true,
        "id": "hky9vV92w5tt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import yaml\n",
        "\n",
        "project_path = os.path.join(extracted_path, \"Coronary_Segmentation_deep_learning\")\n",
        "sys.path.append(project_path)\n",
        "\n",
        "config_path = os.path.join(project_path, \"config/config.yaml\")\n",
        "with open(config_path, \"r\") as config_file:\n",
        "    config = yaml.safe_load(config_file)\n",
        "\n",
        "from model.U_Net import UNet\n",
        "\n",
        "model = UNet(config[\"model\"][\"in_channels\"], config[\"model\"][\"out_channels\"])\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "XIrh4DRlw5tu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from data.Image_loader import ImageLoader\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "image_loader = ImageLoader(config[\"data\"][\"image_dir\"], config[\"data\"][\"mask_dir\"])\n",
        "\n",
        "train_loader = DataLoader(image_loader, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"])\n",
        "\n",
        "for epoch in range(config[\"training\"][\"epochs\"]):\n",
        "    for images, masks in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MuszBlUpw5tu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.Calculate_metrics import calculate_metrics\n",
        "\n",
        "metrics = calculate_metrics(model, train_loader)\n",
        "print(metrics)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uDlGD-JRw5tu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"/kaggle/working/unet_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "results_path = \"/kaggle/working/results.csv\"\n",
        "metrics.to_csv(results_path, index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CCDcJeWow5tv"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}